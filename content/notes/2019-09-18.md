---
title: Learning TF Notes
date: 2019-09-18
tags: TensorFlow, AI
---

I'm trying to familiarize myself with TensorFlow.
Some important questions:

- Figure out what's going on with the dimensions in the layers of a sequential model. I understand that I need to jam "flatten" layers in there to fix it, but what's the idea?
    - The dimensions in the layers seem.. reasonable? Like they reflect the actual data at that point. Then the tensor being passed around during training has an extra dimension for the dataset size..?
- What's an "epoch"? What's a "batch size"? For that matter, what's a "batch"?
    - The batch size is the number of samples to analyze between each step of gradient descent (basically, look at $N$ samples and compute the gradient for the average loss, then step in that direction, then look at the next $N$ samples).
    - An epoch = a run through the entire dataset.
- Transfer learning: train a model one dataset, fix some weights, then train remaining weights on another dataset?In reading biology, there is a view of "coupled reactions" where the idea is to make an endergonic reaction exergonic. How does this go to reaction networks?
