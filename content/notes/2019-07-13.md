---
title: Untitled
date: 2019-07-13
---
Tetlock[^1] describes a process of *extremising* when aggregating
judgments from different sources. If five different people, using
different information, all determine the probability of some event to be
70%, you should judge that probability *more* highly than 70%. More
precisely, you should judge the probability to be *more extreme*,
meaning *further from the base rate*, than 70%. The idea is that each
person is basing their probability on A: the base rate, and B: a
probability update coming from certain information. If they all have
access to different information, then you should update on each of those
pieces of information, resulting in a much more extreme probability than
any individual.

On the other hand, Henrich[^2] describes a process of *conformist
transmission*. Suppose you are trying to figure out which of ten
unfamiliar restaurants is the best. If you are basing your decision on
nothing but the habits of other diners, and you assume they all have the
same taste as you, you will simply go to the most visited one. Thus if
one restaurant has 40% of the visitors, six others each 10%, and three
restaurants are empty, you will not go to one of the less-visited
restaurants 60% of the time.

There is a certain convergence in these ideas. If we interpret the other
diners as making a "probability judgment" about which restaurant is best
(assigning probability $1$ to one and $0$ to the rest), a naive
averaging of their probabilities gives the more popular restaurant only
a 40%. It's not clear how to get from this to extremising using
arguments like the above (if nothing else, we certainly need a more
sopihisticated model of the other diner's probability judgments).

[^1]: <https://en.wikipedia.org/wiki/Philip_E._Tetlock> - see e.g.
    <https://80000hours.org/podcast/episodes/philip-tetlock-forecasting-research/>

[^2]: In
    <https://www.amazon.com/Secret-Our-Success-Evolution-Domesticating/dp/0691166854>
