---
title: Taxonomy of agency
date: 2019-09-06
tags: ML, AI, alignment, optimization
---
Dynamics vs. decision theory.
:   The current [compositional framework](/content/Projects/OpenAgents.md) I'm thinking about is more dynamics-oriented, but I'd like to think more about the decision theory/probability theory side of things.
    Maybe a more thorough description of this dichotomy is what is outline [here](https://www.lesswrong.com/posts/hLFD6qSN9MmQxKjG5/embedded-agency-via-abstraction#mH6woJCNuw8iozuiK)
    I'm not sure these names are good, though - obviously the "dynamic" theory is also a decision theory?

Being *an optimizer* vs being *optimized*

:   See [risks from learned optimization](https://www.alignmentforum.org/posts/FkgsxrGf3QxhfLWHG/risks-from-learned-optimization-introduction) and [utility $\neq$ reward](https://www.lesswrong.com/posts/bG4PR9uSsZqHg2gYY/utility-reward).
    (An *optimizer* will act to increase its utility - but an optimizer created by reinforcement learning is *optimized* to increase its reward - the two are not the same).

The two notions of optimizers: "Agents" and "Optimization methods"

: See [two senses of optimizer](https://www.lesswrong.com/posts/rvxcSc6wdcCfaX6GZ/two-senses-of-optimizer)

More could be added here.
