---
title: Untitled
date: 2019-03-08
---
Expanding on yesterday about group structures: Given a word $w \in FS$,
we clearly get a map $\ev_w: \Hom^F(FS,S) \to S$. (Superscript denotes
the subset spanned by EM-algebras). If two words define the same map,
they are identified in every group structure on $S$, hence equal in
$FS$, so the map $\ev_{(\cdot)}$ is injective. What is the image?

Not always injective, e.g $S=\{a\}$, $w =a, v=a^2$.

Really, a word in $S$ defines a *natural transformation*
$$\Hom(S, U(-)) \to U(-)$$ of functors $Grp \to Set$.

More on probability
-------------------

Let $X$ be a set. A *sample* from $X$ is an element of the free abelian
monoid on $S$, i.e. a map $X \to \N_0$ with finite support. Write the
collection of all samples in $X$ as $S(X)$. Clearly, $S$ defines a
functor $\Set \to \Set$.

We say that two samples $a,b$ are *equivalent* if $n\cdot a = m \cdot b$
for some $n,m \in \N$. Write this as $a \simeq b$.

Let us call an equivalence class of samples on $X$ a *finitary
distribution* on $X$. Write these as $D(X)$. We may identify $D(X)$ with
the collection of map $X \to \Q_{\geq 0}$ with finite support and sum
$1$. It is not hard to see that $D$ is itself a monad. However, note
that it is *not* a quotient monad of $S$. The reason for this is that it
does not preserve the multiplication. The term $(x+x)+y \in S(S\{x,y\})$
is mapped to $x+x+y$ in $S(\{x,y\})$, which then goes to
$\frac{2}{3}x + \frac{1}{3}y$ in $D\{x,y\}$. However, it is mapped to
$\frac{1}{2}x + \frac{1}{2}y$ in the other direction.

A *finitary convex structure* on a set $X$ is the structure of a
$D$-algebra on $X$.

We can think of (real) convex combinations as arising from this idea by
allowing (certain) *limits* of samples. Hopefully, one can derive
non-discrete distributions in the same way, by allowing a limit of
samples including more and more outcomes.

Note the duality: a (discrete) distribution is to convex sets what words
are to groups.

Could study spaces with a distribution and distribution-preserving maps.
Analogue: sets with a word, and word-preserving maps. Category of
elements.

An element of $FS$ is the same thing as a natural transformation
$\Hom(S,-) \to F-$. (covariant Yoneda).

Define a partial ordering on $S(X)$ by $a \leq b$ if $a(x) \leq b(x)$
for all $x$.

Let a *pro-sample* be a directed subset of $S(X)$, i.e each pair of
elements have a common upper bound (not necessarily a least upper
bound).

Given a subset $U \subseteq X$, for each sample $a$, we can define the
*empiric probability of $U$*, $P_a(U)$, as
$$\frac{\sum_{i \in U}a(i)}{\sum_{i \in x}a(i)}$$ We may call the
denominator here the *size* of the sample.

Given a pro-sample $A= \{a\}$ and a subset $U$, we obtain a filter
$\{P_a(U)\}$ in $[0,1]$. If this converges, we call this the probability
of $U$, and denote it $P_A(U)$. We say that $U$ is $A$-measurable.

Note that the collection of $A$-measurable sets is not even stable under
unions, since given two sets in general position, we can find a
pro-sample so that $P_a(U)$ oscillates between $0$ and $1$, while
$P_a(U) = P_a(V) = 1/2$ constantly.

Let $P \subset [0,1]^\N$ be the subset of sequences which sum to $1$. A
$\sigma$-convex space $X$ is a set $X$ equipped with an operation
$\alpha : P \times X^\N \to X$, satisfying these conditions:

(a) If $v_i = 0$, $\alpha(v, \{x_j\})$ is independent of $x_i$

(b) For each finite $N$, let $P_N$ denote the subset of $P$ consisting
    of sequences which are zero after the $N$th index. Then the
    restriction defines a map $P_N \times X^N \to X$ (by (a)), which
    gives a convex structure on $X$.

Maybe there should be some continuity assumptions. We want to axiomatize
convex subset of banach spaces.

Let $X$ be a convex space equipped with a topology. Then $X$ is called a
*convex topological space* if the convex combination operation is
continuous in all coordinates.

In this situation, we say $X$ is *complete* if it satisfies the
following condition: for any sequence $\{x_i\}$ which is contained in a
compact subspace of $X$, and any sequence $r_i$ of positive reals with
sum $1$, the sequence
$\sum_{i=1}^N r_i x_i + (\sum_{i=N+1}^\infty r_i) y$ converges to an
element which we denote $\sum_{i=1}^\infty r_i x_i$, which is
independent of $y$.

The compactness assumption is necessary to ensure that Banach spaces are
complete (by bounding the norm). However, it means that we cannot take
all the convex combinations we may want to (for instance, if $x_i \to x$
in a banach space and $\{x_i\} \subset B(0,1)$, but $x \notin B(0,1)$,
no compact subset of $B(0,1)$ contains all the $x_i$, but we would still
like to take convex combinations of them in $B(0,1)$).

A *mean space* is an algebra for the Giry monad, i.e a measure space $X$
equipped with a map $GX \to X$ satisfying some equations.

What we really want is an axiomatic description of this.

We want to describe a sigma algebra on $\Hom(X,Y)$ for measurable space
$X,Y$. But this seems to run into the issue that maps should not be
defined pointwise? Can be avoided by working with continuous maps.. but
is this too restrictive?

A $\sigma$-lattice is a Boolean algebra with countable meets. A
*measure* on a $\sigma$-lattice is a map into $\R_{\geq 0}$ which is
countably additive in the obvious sense, and maps $\bot$ to $0$.

Given a measurable space $(X,\Sigma)$, $\Sigma$ with the inclusion
ordering is a $\sigma$-lattice. A measure on $(X,\Sigma)$ is precisely a
measure on $\Sigma$ in this sense.

Given a measure $\mu$ on a $\sigma$-lattice $A$, say two elements
$x,y\in A$ are equivalent if
$\mu( (x \wedge -y) \vee (-y \wedge x))  = 0$. One can check that the
quotient by this relation defines a new $\sigma$-lattice $A/\mu$, and
that $\mu$ descends to a measure on this $\sigma$-lattice, with the
property that $\mu(x) = 0 \Rightarrow x = \bot$ in $A/\mu$.

You should view a $\sigma$-lattice as a measure space after quotienting
by null sets.

The category $\Sigma$ has objects the $\sigma$-lattices, and maps the
obvious things. (note that maps must also preserve $\top, \bot$)

We would hope for $\Sigma^{op}$ to be a topos.

1.  $\Sigma$ is clearly locally small, and the lattice $\{\bot,\top\}$
    is a (co) generator? It would suffice that given $a\neq b \in X$,
    there exists a map to $2$ that separates them. Unclear?

2.  $\Sigma$ h

A statistical context can be thought of as a co-presheaf on finite (or
countable) sets where $F(X)$ is the collection of experiments with
outcome in $X$ one could do (or measurements with value in $X$). Then
the conditioning structure seems to come from subset inclusions
$X_0 \into X$ - given that $e \in F(X)$ has its outcome in $X_0$, what
do we learn?

Given a variable $y \in F(Y)$ which we want to know about, and an
experiment $x \in F(X)$ which we can measure, we can learn about $y$ by
forming the product $(x,y) \in F(X) \times F(Y) \simeq F(X \times Y)$
and conditioning on $\{x_0\} \times Y \subset X \times Y$

(Product structure - ability to make *simultaneous measurements*, and
reason about them, *joint distributions*, seems important)

For a countable set $X$, let $D(X)$ denote the probability distributions
on it.

An *abstract statistical context* consists of the following data:

1.  A co-presheaf, $E$, on the category of finite (countable?) sets.

2.  Which is a co-sheaf with respect to the co-topology generated by
    jointly injective maps (so it preserves finite limits).

3.  For each subset inclusion $X_0 \into X$, a map $E(X) \to E(X_0)$.

A *probability assignment* consists of a collection of maps
$$EX \to DX$$ which are natural with respect to the action on maps and
on inclusion. (Need to figure out conditioning on zero? Or just say
surjective maps and prob. distributions with full support?)

The naturality with respect to subset inclusions means that Bayes'
theorem has to hold.

Let $x \in EX, y \in EY$ be experiments, and let $x_0$ be a point in
$X$. By definition $E(X\times Y) \simeq EX \times EY$, so we may
consider $(x,y) \in E(X \times Y)$. Now by conditioning on the inclusion
of $Y \simeq \{x_0\} \times Y$, we obtain a map $E(X\times Y) \to EY$.
We define $y | x=x_0$ to be the image of $y$ under this map.

This makes abstract sense, but it's not clear how it's useful in any
way, in the sense of being a useful framework for statistics or
learning.
