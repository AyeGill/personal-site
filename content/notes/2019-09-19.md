---
title: Cognitive Science, Misc
date: 2019-09-19
tags: Cognitive Science, Questions
---
## Cognitive Science Notes 7

Reading chapter 10 "How are cognitive systems organized?"

We first encounter various agent architectures from AI, with a discussion of how they compare to each other and to "real" cognition.
A common feature is that they are described in terms of specialzied submodules.
This motivates a question: Can we think of the human brain in terms of submodules, and how can we recognize them?
This leads to Jerry Fodor's idea of what he calls *cognitive modules*.
He contrasts this with what he calls "horizontal decomposition", which is breaking cognition apart into memory, attention, etc.
What makes it horizontal is that these faculties are *domain-general*.
In contrast, he presents a theory of *vertical* decomposition, where the mind is broken into modules that are

- Domain-specific
- Informationally encapsulated
- Have mandatory application (you can't choose *not* to process the signal from the retina using your visual center. And you don't stop seeing optical illusions even when you know them to be illusions).
- Fast.

Two more "optional" features of mental modules are

- Fixed neural architecture - they're often located in specific bits of the brain
- Specific breakdown patterns - they often fail in specific interesting ways when damaged or "tricked".

Some examples of tasks that Fodor belives are likely carried out by mental modules:

- Color perception
- Shape analysis
- Analysis of 3d spatial relations
- Visual guidance of motion
- Face recognition
- Gramattical analysis of heard speech
- Rhythmic or melodic structure detection
- Recognizing the voices of other humans.

Fodor believes these modules are linked together by "central processing", which is domain-general and context-specific. According to him, this makes it a poor fit for analysis in computational terms in specific, and for analysis by cognitive science in general.

## Misc

Utilitarian Kalman filter?
A Kalman filter incorporates a (fixed) model of how the state of the world evolves - what sort of toy model of learning can we get by adapting it to optimize a function of state?
Adding a regularizer to the error term when doing stochastic gradient descent = weight decay
