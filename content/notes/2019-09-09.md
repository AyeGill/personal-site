---
title: The power in noise
date: 2019-09-09
tags: game theory
---

See [The Weighted Majority Algorithm](https://www.lesswrong.com/posts/AAqTP6Q5aeWnoAYr4/the-weighted-majority-algorithm).

Suppose we are writing a program $P$, which does something interesting depending on an "input state" from a set $A$.
It doesn't matter exactly what it does, the important thing is that it has a certain performace, which may depend on the input and the actual choice of program.
Maybe the program is computing a complex function of the input, and performance is (negative) running time. Or it's a decision-theoretic problem and the performance is built into the problem as a utility function. The program does not necessarily know the "input state" - for instance, in the post I linked above, the "input state" consists of the set of questions, the correct answers in each case, as well as each expert's answer in each case.

There are, naively, two factors that we could reasonably try to optimize.

- Given a probability distribution on $A$, we can try to maximize average-case performance.
- We can try to find a program which maximizes the worst-case performance.

(We could also optimize best-case performance, or many other things, but the above two seem like the most reasonable targets).

In the first case, we can write the following simple proof that randomness cannot help performance:
Let the performance of (deterministic) program $x$ in state $a \in A$ be $U(x,a)$.
A random algorithm is equivalent to a probability distribution on deterministic programs.
Given such a thing, the expected performance is $U = \sum_x \sum_a U(x,a)p_x(x)p_a(a)$.
It is clear that at least one algorithm $x_0$ must have $\sum_a U(x_0,a) p_a(a) \geq U$.

In the second case, this proof doesn't work.
Why? Here is an explanation which isn't phrased in adversarial language: because different $x$s have different worst-case performance, so taking a weighted average can improve the worst-case performance.
But this explanation is kind of cheating: we are taking the $a$ which gives the worst-case performance, but not the $x$.
This seems like an odd choice - if we are allowed to do this sort of probabilistic reasoning on $x$, why not on $a$?

One justification for it is the adversarial one: if the distributions of $a$s *actually depends* on the chosen algorithm, but not on the output of our random number generator,
then adding this noise allows us to "spread out" performance among different inputs, shoring up the worst-case.

Another potential justification is robustness to our own mistakes. If we make a bad mistake when putting a distribution on $A$, we could choose an algorithm with terrible performance in reality.
Choosing an algorithm at random, in such a way that we can provide a lower bound on performance *independent* of the distribution on $A$, is a way to safeguard against this.
But (although I have not proved it) it seems to me that this type of "safeguarding" could be accomplished also by some combination of

- Using a measure of performance which penalizes bigger errors more severely (if we are really interested in lower bounds, it must be because they are valuable..)
- Using a more entropic prior
