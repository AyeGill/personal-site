---
title: Stein's Paradox, Cognitive Science
date: 2019-09-16
tags: Cognitive Science, Statistics
---

## Stein's Paradox

From [here](https://normaldeviate.wordpress.com/2013/05/18/steins-paradox/).

Let $X = \cN(\theta,1)$, and let $\hat{\theta}: \bR \to \bR$ be an estimator, ($\hat{\theta}(X) \approx \theta$).
Then the \emph{risk} of $\hat{\theta}$ is $R_\theta(\hat{\theta})\bE(\theta - \hat{\theta})^2$, the expected squared error.
We say an estimator is *admissible* if there is no estimator with less risk, i.e $R_\theta(\hat{\theta}) \geq R_\theta(\hat{\theta}')$ with strict equality for at least one $\theta$.
This obviously makes sense for higher-dimensional distributions as well.

::: Theorem :::

In dimension $k$, $\hat{\theta} = X$ is an admissible estimator if and only if $k\leq 2$.

:::

This is *Stein's paradox*.
The essential idea is that we can get a better estimator by "regularizing" by shrinking the estimates towards the origin[^or], for instance by
$\theta* = \left(1 - \frac{k-2}{\abs{X}^2}\right) X$.
(The *James-Stein estimator*).

[^or]: Or any other point, it doesn't really matter.

This is trading increased bias for decreased variance.

## Cognitive Science Notes 5

We encounter three incarnations of the physical symbol hypothesis

- SHAKEY, a robot which can make simple plans and move about the environment. It uses a predicate calculus model of the world, along with rules describing how each action updates this world, to make plans, essentially by searching for proofs.
- WHISPER, a system which analyzes a simple block word to determine how the blocks will collapse. Interesting in that it uses its visual system in a key way to analyze the collapse.
- ID3, a machine learning algorithm which, given a large dataset, finds a decision tree to predict a target value based on the other values.
  
## Misc/Links

[This comment](https://www.lesswrong.com/posts/S3W4Xrmp6AL7nxRHd/formalising-decision-theory-is-hard?_ga=2.154643064.1926787476.1568638368-341328662.1566910481#X7R4rxHpkEKycJvSb) about reframing decision theory in terms of learning.
Essentially, even though RL "feels" more like Causal Decision Theory, it will learn the correct move if it plays Newcomb repeatedly.
The point is that a decision theory is conditioned on a model of the external world, but this problem has to be learned somehow - and if you learn a model of Newcomb by playing it many times, your model will say that one-boxing "causes" the large payout.
Found in the context of [Do sufficiently advanced agents use logic](https://www.alignmentforum.org/posts/3qXE6fK47JhSfkpnB/do-sufficiently-advanced-agents-use-logic).