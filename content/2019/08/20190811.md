---
title: Sunday, 2019-08-11 {#2019811:S}
---
A very trivial neural network gradient descent example. Let's suppose we
are learning a linear function $\R^2 \to \R$, parameterized by $a,b$,
$(x,y) \mapsto ax+by$. Suppose we get the training datum $(1,-1),5$, and
$a=b=1$. (Suppose we're using squared-distance loss). Then the loss is
$(a,b) \mapsto (a-b-5)^2$, so the gradient at $(1,1)$ is $(-10,10)$ In
other words, because $a$ counts positively at the current input, and
we're too low, we step in a positive $a$-direction.

I want an extension of the language of Fong-Spivak-Tuyeras where we can
define GAN using this picture:

![](/images/d09e721a79d49c9a14c671bf617cca77d047af30.svg)

Of course it has to be time-dependent somehow, when we have a loop of
neural networks like this, we are not using a fixpoint operator. So
maybe we should label the diagram:

![](/images/a6b06737829a279dc19bd7375b9100a2f666337b.svg)
