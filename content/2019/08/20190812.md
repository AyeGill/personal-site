---
title: Monday, 2019-08-12 {#2019812:S}
---
I am reading "Compositional Game Theory" by Gahni-Heges-Winschel-Zahn
[@GHWZ18] I think maybe the duality here could be a way of doing what I
wrote about yesterday The problem is that we don't have
"comultiplications". Maybe this is where we need delays?

The category of neural network architectures (as in Fong-Spivak-Tuyeras)
could maybe be described as a free symmetric monoidal category with
bimonoids\... of some type. The idea would be that the functor
$NNet \to Learn$ is determined uniquely by what it does to a neuron (i.e
saying that each connection is given by a weight, specifying the
activiaton and error functions and the step size).

The idea is that a $1$-layer neural network can be decomposed into some
comultiplications(forks), some multiplications(joins), and some wires
(which carry weights). This would mean that $NNet$ is "generated" by one
object, $1$, an endomorphism $1 \to 1$, corresponding to multiplication
by a parameter. (There is some confusion about where the activation
function comes into this).

In Haine's paper [@Haine19], we see how to construct the $(-)^+$-functor
of Lurie, the right adjoint to
$\tau_{\leq 0}: preTop_\infty \to preTop$. It is given by
$X^+ = Sh_{eff}(X)^{coh}_{<\infty}$ - in other words, by transporting
the adjunction
$$\tau_{\leq 0}Top^{bc}_\infty \rightleftharpoons Top^{coh} : i$$
(coming from the equivalence between coherent topoi and coherent
$1$-localic $\infty$-topoi) along the equivalences (or rather,
dualities) between (bounded and) coherent topoi and pretopoi.
