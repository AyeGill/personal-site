---
title: Thursday, 2019-08-15 {#2019815:S}
---

We modify the definition of open game, essentially to make updates
"forced" (or maybe deterministic is a better description?).

An *open agent* $A: (X,S) \to (Y,R)$ consist of:

1.  A set $\Sigma_A$ of *strategies* (internal states).

2.  A *decision function* $d_A: \Sigma_A \times X \to Y$.

3.  A *codecision function* $c_A: \Sigma_A \times X \times R \to S$.

4.  A *update function*
    $u_A: X \times (Y \to R) \times \Sigma_A \to \Sigma_A$

Given agents $A: (X,S) \to (Y,R), B: (Y,R) \to (Z,T)$, we define their
composition $A;B$ as:

1.  $\Sigma_{A;B} = \Sigma_A \times \Sigma_B$.

2.  $d_{A;B}((s_a,s_b),x) = d_B(s_b,d_A(s_a,x))$.

3.  $c_{A;B}((s_a,s_b),x,t) = c_A(s_a,x,c_B(s_b,d_A(s_a,x),t))$

4.  $u_{A;B}(x,k,(s_a,s_b)) = (u_a(x,k',s_a),u_b(d_A(s_a,x),k,s_b))$,
    where $k': Y \to R$ is given by $k'(y) = c_B(s_b,y,k(d_B(s_b,y)))$

There is nothing really new here compared to the paper - the only
difference is we require the "optimal response" relation to be a
function, and curry this.

Let us suppose given a certain function $f:X \to Y$, and a loss function
$l:Y\times Y \to \bR$. Let us also take for granted a utility-optimizing
agent who observes $X$ and decises a $Y$ - in other words, a learner
$A: (X,1) \to (Y,\bR)$. Let us avoid assuming that the update is always
to absolutely maximize utility of the current input, as in [@GHWZ18] -
this is not even a well-defined function. Let us merely take
"utility-optimizing" to say that the update function is "trying" to make
the utility go up.

Then we can make this agent learn the function $f$ by pasting in a "loss
circuit" based on $l$ and $f$, as in this figure:

![](/images/98a4aace817097a8c1365acef00d39a39159607b.svg)

Here $L(x,y) = l(f(x),y)$. This agent consists of a set $\Sigma_A$, a
map $\Sigma_A \times X \to Y$, and an update function
$\Sigma_A \times X \to \Sigma_A$, which attempts to minimize loss.

One could similarly have separate sources of $X$ and $Y$, if we want to
train on a dataset of pairs, and not a specific given function.

I think this paradigm is sufficient to encompass GAN - my misgivings
about "time-depence" are solved by the fact that the "loop" is only in
utility, not in observations. I.e, the observations of the discriminator
depend on the generator, but not vice versa. So this is doable with open
games.

We may say that this framework allows for agent which *care about each
other*, in the sense that their utilities may depend on the other
agents, but it does not allow for agents which *interact*, in the sense
that their inputs depend on each other in a cyclical way

But this is not quite true - consider e.g. the agent $(1,1) \to (X,X)$
with internal state $X$, which plays its state and updates according to
the utility function. In fact, this "parrot" seems to play exactly the
role of the time-delay, since it takes an update step to change its
state. Moreover, if wired into a network of other agents, it plays
exactly the role expected during an update of the entire network - it
spits out the current value for the purposes of other updates, and uses
the rest of the network as the update function - in other words,
whatever comes back is what it starts spitting out.

I eventually found [@BHW18], which seems to have had a very similar
idea. Instead of considering "strategy updates" in my style, they
consider simply a set of *equilibrium strategies*. Then a strategy
$x \in X$ is equilibrium if it is a fixpoint of the context. It seems
quite obvious to have a similar motivation - of studying how to "let the
arrows bend the other way" in the category of games - but the flavor
here seems to be more economical (pinpointing equilibrium strategies)
than algorithmical/behavioural (describing how agents interact and
update their strategies).
