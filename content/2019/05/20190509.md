---
title: Thursday, 2019-05-09 {#2019509:S}
---
Ultracategories and Pyknotic categories
---------------------------------------

Let $\Ult^L$ denote the `\icat `{=latex}of ultra-`\icats `{=latex}and
left ultrafunctors. Define a functor $\Ult^L \to \Pyk(\Cat_\infty)$ by
$\cl{C} \mapsto \Fun^{\LUlt}(-,\cl{C})$.

Conjecture:

1.  This functor is fully faithful, even in the $(\infty,2)$-categorical
    sense.

2.  The essential image consists of those pyknotic
    `\icats `{=latex}$\cl{C}(-)$ with the property that
    $$\cl{C}(\beta X) \to \cl{C}(\ast)^X$$ admits a right adjoint for
    each (tiny) set $X$, with the property that $LR \simeq 1_\cl{C}^X$.

The idea is that a *left* ultrafunctor $F: \beta X \to \cl{D}$ is an
actual *ultrafunctor* iff it's terminal in the fiber over
$F: X \to \cl{D} \in \cl{D}^X$.

To see this, we observe the diagram

![](/images/0214a13cc4abb841def0458124c58ced2315c735.svg)

If $F,F'$ are extensions of $F: X \to \cl{D}$ to left ultrafunctors
$F: \beta X \to \cl{D}$, a map $F' \to F$ lying over the identity is a
choice of dashed arrow. Such an arrow is unique if the bottom arrow is
an isomorphism. Hence there is a unique such $F$, and we can recover the
ultraproduct $\int_X F(X)d\mu$ as $F(\mu)$, thus reconstructing the
ultrastructure from these functors. On the other hand, the existence of
a functorial initial object in each fiber is precisely the statement
that such an adjoint exists.

Idea:

We interpret Makkai conceptual completeness as saying that functoriality
and preservation of ultraproducts is exactly what it takes for a family
of sets to be definable. There is an intuition here that the
ultraproducts clause is the "infinitary" condition needed to make this
work. Can one make sense of this?

Learning algorithms
-------------------

`\index{Machine Learning}`{=latex} I am typing some stuff from
Spivak-Fong(??) on backprop as functor from memory. May be mixing in own
ideas: A (supervised) learning process for functions $A \to B$ consists
of: A set $P$ parameterizing the functions in question. An
"implementation map" $P \times A \to B$. An update map
$P \times A \times B \to P$ (current state + input/output pair -\> new
state) And a backpass map $P \times B \to A$ \[???\] which is necessary
to compose learning algorithms.

What is the categorical abstraction here?
